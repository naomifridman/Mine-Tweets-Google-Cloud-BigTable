{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Tweet About BigData ?\n",
    "### Collect the words people Tweet, when they Tweet about BigData\n",
    "Find highest frequency words among Tweets about, for example, BigData in different locations.\n",
    "\n",
    "## Project goals:\n",
    "#### Project goal is to findinf most popular word in Tweet's about a given subject, and compare the results from different locations. I used BigData as an example, and added a political view analyzis as well :). In the same way, you can analize tweets about any subject.\n",
    "#### The repository contais Hadoop MapReduce word count and top n words functionality, to process the collected words from Tweeter. Here we use Python Collection library, for demonstration.\n",
    "### In this Python notebook, we:\n",
    "* Implement Tweets retrieving, according to a given query.\n",
    "* Reserch Tweet data structure.\n",
    "* Reserch Tweets location issues.\n",
    "* Implement word tokenizing and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\yoav\n",
      "[nltk_data]     fridman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy   #Python library for accessing the Twitter API.\n",
    "\n",
    "import nltk     #Python NLP library for tokenizing tweeter text\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Twitter API Authentication \n",
    "In order to extract tweets, we need an access kyes to Twitter API. To get those, you need a twitter acount, and you need to create a Twitter App to get the following 4 keys:\n",
    "\n",
    "* Consumer Key (API Key)\n",
    "* Consumer Secret (API Secret)\n",
    "* Access Token\n",
    "* Access Token Secret\n",
    "\n",
    "Write those kyes in a text file, each in separate row. name the file: auth.k\n",
    "Website to create the Twitter App https://apps.twitter.com/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHENTICATION (OAuth)\n",
    "def tw_oauth():\n",
    "    authfile = 'auth.k'\n",
    "    with open(authfile, \"r\") as f:\n",
    "        ak = f.readlines()\n",
    "    f.close()\n",
    "    auth1 = tweepy.auth.OAuthHandler(ak[0].replace(\"\\n\",\"\"), ak[1].replace(\"\\n\",\"\"))\n",
    "    auth1.set_access_token(ak[2].replace(\"\\n\",\"\"), ak[3].replace(\"\\n\",\"\"))\n",
    "    return tweepy.API(auth1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tweet data structure\n",
    "Lets extract 5 recent tweets, and view the tweet data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 recent tweets about Bigdata:\n",
      "RT @AiFinTek: AIRDROP Alert -Crypto project live, Free Tokens,100$ AFTK TOKEN,50$ AFTK Friend Referral, https://t.co/Hy9MZ5jfLS #ICO #block…\n",
      "RT @jblefevre60: What are the #Top10 #Disruptive #technologies?\n",
      "\n",
      "#fintech #Digital #Blockchain #AI #AR #VR #Drones #BigData #IoT #Robots #3…\n",
      "RT @IainLJBrown: Accelerating big data for social good with UNICEF\n",
      "\n",
      "Read more here: https://t.co/kMc9w7n63p\n",
      "\n",
      "#BigData #DataScience #Machine…\n",
      "RT @MarcoPark21: RT DeepLearn007 \"RT DeepLearn007: Stanford University: Deep Learning Comes Full Circle\n",
      "#AI #MachineLearning #DeepLearning…\n",
      "How Is #BigData Influencing the Education Sector? #AI #ML  https://t.co/jMpsNVIWLE\n"
     ]
    }
   ],
   "source": [
    "example_tweet = None\n",
    "\n",
    "api = tw_oauth()\n",
    "print ('5 recent tweets about Bigdata:')\n",
    "Counter = 0\n",
    "for tweet in tweepy.Cursor(api.search, q = 'Bigdata',lang = 'en',count = 5).items(): \n",
    "    print (tweet.text)\n",
    "    Counter += 1\n",
    "    if (Counter == 5):\n",
    "        example_tweet = tweet \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tweet data is an object containing information about the authour and the Tweet.Lest view few fildes that is relevant to us.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:  994303915171565568\n",
      "created_at: 2018-05-09 19:51:41\n",
      "geo: None\n",
      "coordinates None\n",
      "text: How Is #BigData Influencing the Education Sector? #AI #ML  https://t.co/jMpsNVIWLE\n",
      "geo_enabled: True\n"
     ]
    }
   ],
   "source": [
    "# We print info from the first tweet:\n",
    "print('id: ', example_tweet.id)\n",
    "print('created_at:', example_tweet.created_at)\n",
    "print('geo:',example_tweet.geo)\n",
    "print('coordinates',example_tweet.coordinates)\n",
    "\n",
    "print('text:',example_tweet.text)                   #tweet text\n",
    "print('geo_enabled:',example_tweet.author.geo_enabled)     #is author/user account geo enabled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenize tweet\n",
    "** Lest look at tweet test example:**<br>\n",
    "RT @Hiredscore: Our team is gearing up for @Littler's 35th annual Executive Employer Conference in Phoenix, AZ this week! Attending the con…<br>\n",
    "** To analize commom words in tweet, We need to tokenize and filter tweet text**<br>\n",
    "* we change all words to lower case not to have doubles\n",
    "* We will use nltk library for tokenizing\n",
    "* we will remove stop words a, is, the...\n",
    "* we will remove unintersting words, specific to tweeter, as: RT, &amp\n",
    "* nltk tokenizing separate @ and # from the words, so if we limit word size to >1, its good enought.\n",
    "* We will get user_id as a word in our list, since nltk splits @ronbon to @ and ronbon, but its interseting to the users that tweet so much about the given subgect.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of stop words, words we dont want to appear in our list.\n",
    "s_words = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', \n",
    "              'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be',\n",
    "              'some', 'for', 'do', 'its', 'yours','de', 'vs', 'such', 'into', 'of', 'most', 'itself',\n",
    "              'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each',\n",
    "              'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', \n",
    "              'through', 'don', '\\'\\'','nor', 'me', 'were', 'her', 'more', 'himself',\n",
    "              'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', \n",
    "              'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', \n",
    "              'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on',\n",
    "              'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why',\n",
    "              'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has',\n",
    "              'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after',\n",
    "              'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by',\n",
    "              'doing', 'it', 'how', 'further', 'was', 'here', 'than', 'The',\n",
    "              'rt', '&amp', ' ', '','``', 'http', 'via', 'https', 'amp','\\'s'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize_tweet_text(tweet_text, Qye_words = None):\n",
    "    \n",
    "    word_tokens = word_tokenize(tweet_text)\n",
    "\n",
    "    filtered_sentence = []\n",
    "\n",
    "    for w in word_tokens:\n",
    "        if w.lower() not in s_words | set(Qye_words):\n",
    "            if (len(w)<= 1): continue\n",
    "            if ('\\\\' in w): continue\n",
    "            if ('/' in w): continue\n",
    "                \n",
    "            if (w.endswith('...')): w = w[:-4]\n",
    "            if (w.startswith('...')): w = w[4:]\n",
    "            if (w.endswith('-')): w = w[:-1]\n",
    "            if (w.startswith('-')): w = w[1:]\n",
    "            if (w.endswith('…')): w = w[:-1]\n",
    "            if (w.startswith('…')): w = w[1:]\n",
    "            if (w.isdigit()): continue\n",
    "            filtered_sentence.append(w.lower())\n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['april', 'jax', 'magazine', 'machine', 'learning', 'bigdata', 'deeplearning', 'machineleaning', 'datascience', 'ai', 'python', 'rstats', 'filtration']\n"
     ]
    }
   ],
   "source": [
    "# Lets check the tokenizing function\n",
    "\n",
    "example_sent = \"April 2018 JAX Magazine is Out: Machine Learning. #BigData #DeepLearning #MachineLeaning #DataScience #AI #Python #RStats @filtration \\\\ppo.\"\n",
    "word_tokens = tokenize_tweet_text(example_sent, Qye_words = ['BigData'])\n",
    "\n",
    "print(word_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrive Tweet data definitions\n",
    "There is a lot of data associated with each tweet. Lest view few the search fieldes that is relevant to our task.<br>\n",
    "refernce: https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators\n",
    "* **q\trequired**\tA UTF-8, URL-encoded search query of 500 characters maximum, including logical operators operators. <br>\n",
    "    example: <br>\n",
    "    * q = 'bigdata' : will retrive Tweets that have the word bigdata\n",
    "    * q = 'puppy filter:media' : will retrive Tweets containing “puppy” and an image or video.\n",
    "* **geocode\toptional**\tReturns tweets by users located within a given radius of the given latitude/longitude. The location is preferably taken from the Geotagging API, but will fall back to their Twitter profile. The parameter value is specified by ” latitude,longitude,radius ”, where radius units must be specified as either ” mi ” (miles) or ” km ” (kilometers). Note that you cannot use the near operator via the API to geocode arbitrary locations; however you can use this geocode parameter to search near geocodes directly. A maximum of 1,000 distinct “sub-regions” will be considered when using the radius modifier.<br>\n",
    "    example: 37.781157 -122.398720 1mi\n",
    "* **lang\toptional**\tRestricts tweets to the given language, given by an ISO 639-1 code. Language detection is best-effort. We will use only english<br>\n",
    "    example: eu, en\n",
    "* **count\toptional**\tThe number of tweets to return per page, up to a maximum of 100. Defaults to 15. This was formerly the “rpp” parameter in the old Search API.<br>\n",
    "    example: 100\n",
    "<br>\n",
    "Full definition can be found here: https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 How can we Filter Tweet by location\n",
    "\n",
    "\n",
    "When working with Tweet data, there are two classes of geographical metadata:<br>\n",
    "\n",
    "* **Tweet location** - Available when user shares location at time of Tweet.\n",
    "* **Account Location** - Based on the ‘home’ location provided by user in their public profile. This is a free-form character field and may or may not contain metadata that can be geo-referenced.\n",
    "Nullable . The user-defined location for this account’s profile. Not necessarily a location, nor machine-parseable. This field will occasionally be fuzzily interpreted by the Search service. Example: \"location\": \"San Francisco, CA\"\n",
    "\n",
    "**Important Notes:**\n",
    "\n",
    "    * Geographical coordinates are provided in the [LONG, LAT] order. <br>\n",
    "The one exception is the deprecated  ‘geo’ attribute, which has the reverse [LAT, LONG] order.\n",
    "\n",
    "    \n",
    "#### Tweet locations (\"geo-tagged\" Tweets)\n",
    "Twitter enables users to specify a location for individual Tweets. Tweet-specific location information falls into two general categories:\n",
    "\n",
    "    * Tweets with a specific latitude/longitude “Point” coordinate\n",
    "    * Tweets with a Twitter “Place” \n",
    "Tweets with a Point coordinate come from GPS enabled devices, and represent the exact GPS location of the Tweet in question. This type of location does not contain any contextual information about the GPS location being referenced (e.g. associated city, country, etc.), unless the exact location can be associated with a Twitter Place.\n",
    "\n",
    "Tweets with a Twitter “Place” contain a polygon, consisting of 4 lon-lat coordinates that define the general area (the “Place”) from which the user is posting the Tweet. Additionally, the Place will have a display name, type (e.g. city, neighborhood), and country code corresponding to the country where the Place is located, among other fields.\n",
    "\n",
    "#### Autor location \n",
    "Author location is an an arbitary centence that describe the author location. It is saved in authorloc varible. Author location for example can be:\n",
    "* Hillingdon, London\n",
    "* NY USA\n",
    "Or any other centence the user chose to use as location description.\n",
    "** To filter our desired locations, we will check if strings such as NY or London are contained in the authorlocation filed.\n",
    "\n",
    "## Conclusions:\n",
    "### From our tests and Tweeter documentation, not more then 0.5% of the tweets has geo tagging, so to get the full picture, we will filter tweet locations by the description in the Author location field.\n",
    "\n",
    "## 4.2 Filter tweets by time\n",
    "* **since**\toptional filed in tweeter retriving. It Filter tweets created from the given date. Date should be formatted as YYYY-MM-DD. **Keep in mind that the search index has a 7-day limit**. In other words, no tweets will be found for a date older than one week!!.<br>\n",
    "    example: 2015-07-19\n",
    "\n",
    "source: https://developer.twitter.com/en/docs/tutorials/filtering-tweets-by-location <br>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweets since:  2018-4-9\n"
     ]
    }
   ],
   "source": [
    "# Define start date of our word survey\n",
    "import datetime\n",
    "delta = -30\n",
    "start_date = datetime.datetime.now() + datetime.timedelta(delta)\n",
    "start_date = str(start_date.year) + '-' + str(start_date.month) + '-' + str(start_date.day)\n",
    "print('Collecting tweets since: ', start_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweets since:  2018-4-9\n"
     ]
    }
   ],
   "source": [
    "print('Collecting tweets since: ', start_date)\n",
    "# TWEEPY SEARCH FUNCTION\n",
    "# refernce: https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets\n",
    "def tw_get_tweets(api,  query_in, Qye_words, geo, location, num_tweets=200):\n",
    "\n",
    "    counter = 0\n",
    "    example_tweet = None\n",
    "    word_list = []\n",
    "    \n",
    "    # We will not use geocode inorder not to miss most of the tweets.\n",
    "    for tweet in tweepy.Cursor(api.search,\n",
    "                                q = query_in,           # the actual words we search\n",
    "                                #geocode = geo,          # location\n",
    "                                since = start_date,\n",
    "                                count = num_tweets).items():\n",
    "\n",
    "        #TWEET INFO\n",
    "        created = tweet.created_at             #tweet created\n",
    "        text    = tweet.text                   #tweet text\n",
    "        tweet_id = tweet.id                    #tweet ID# (not author ID#)\n",
    "        cords   = tweet.coordinates            #geographic co-ordinates\n",
    "        geo_e   = tweet.author.geo_enabled     #is author/user account geo enabled?\n",
    "        place   = tweet.place\n",
    "        authorloc = tweet.author.location      #author/user location\n",
    "        \n",
    "        if any(word in authorloc for word in location):\n",
    "            word_list += tokenize_tweet_text(text, Qye_words = Qye_words)\n",
    "\n",
    "        counter = counter +1\n",
    "        if (counter >= num_tweets):\n",
    "            break\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Lets Run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retreiving tweets since:  2018-4-9  about bigdata  Tweeted in locations:  NY\n",
      "Retreiving tweets since:  2018-4-9  about bigdata  Tweeted in locations:  London\n",
      "Retreiving tweets since:  2018-4-9  about bigdata  Tweeted in locations:  Mumbai\n",
      "Retreiving tweets since:  2018-4-9  about bigdata  Tweeted in locations:  Paris\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NY_words</th>\n",
       "      <th>NY_freq</th>\n",
       "      <th>London_words</th>\n",
       "      <th>London_freq</th>\n",
       "      <th>Mumbai_words</th>\n",
       "      <th>Mumbai_freq</th>\n",
       "      <th>Paris_words</th>\n",
       "      <th>Paris_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iot</td>\n",
       "      <td>29</td>\n",
       "      <td>ai</td>\n",
       "      <td>33</td>\n",
       "      <td>machinelearning</td>\n",
       "      <td>2</td>\n",
       "      <td>data</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ai</td>\n",
       "      <td>25</td>\n",
       "      <td>iot</td>\n",
       "      <td>25</td>\n",
       "      <td>kirkdborne</td>\n",
       "      <td>2</td>\n",
       "      <td>read</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data</td>\n",
       "      <td>15</td>\n",
       "      <td>read</td>\n",
       "      <td>23</td>\n",
       "      <td>aftk</td>\n",
       "      <td>2</td>\n",
       "      <td>iainljbrown</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cybersecurity</td>\n",
       "      <td>13</td>\n",
       "      <td>data</td>\n",
       "      <td>21</td>\n",
       "      <td>datascience</td>\n",
       "      <td>2</td>\n",
       "      <td>big</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tech</td>\n",
       "      <td>13</td>\n",
       "      <td>iainljbrown</td>\n",
       "      <td>17</td>\n",
       "      <td>falling</td>\n",
       "      <td>1</td>\n",
       "      <td>datascience</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>digital</td>\n",
       "      <td>13</td>\n",
       "      <td>datascience</td>\n",
       "      <td>17</td>\n",
       "      <td>project</td>\n",
       "      <td>1</td>\n",
       "      <td>ai</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fisher85m</td>\n",
       "      <td>11</td>\n",
       "      <td>machinelearning</td>\n",
       "      <td>16</td>\n",
       "      <td>love</td>\n",
       "      <td>1</td>\n",
       "      <td>machinelearning</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vladobotsvadze</td>\n",
       "      <td>11</td>\n",
       "      <td>analytics</td>\n",
       "      <td>15</td>\n",
       "      <td>block</td>\n",
       "      <td>1</td>\n",
       "      <td>iot</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>analytics</td>\n",
       "      <td>10</td>\n",
       "      <td>google</td>\n",
       "      <td>12</td>\n",
       "      <td>short</td>\n",
       "      <td>1</td>\n",
       "      <td>ml</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vr</td>\n",
       "      <td>10</td>\n",
       "      <td>vladobotsvadze</td>\n",
       "      <td>11</td>\n",
       "      <td>alert</td>\n",
       "      <td>1</td>\n",
       "      <td>artificialintelligence</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         NY_words  NY_freq     London_words  London_freq     Mumbai_words  \\\n",
       "0             iot       29               ai           33  machinelearning   \n",
       "1              ai       25              iot           25       kirkdborne   \n",
       "2            data       15             read           23             aftk   \n",
       "3   cybersecurity       13             data           21      datascience   \n",
       "4            tech       13      iainljbrown           17          falling   \n",
       "5         digital       13      datascience           17          project   \n",
       "6       fisher85m       11  machinelearning           16             love   \n",
       "7  vladobotsvadze       11        analytics           15            block   \n",
       "8       analytics       10           google           12            short   \n",
       "9              vr       10   vladobotsvadze           11            alert   \n",
       "\n",
       "   Mumbai_freq             Paris_words  Paris_freq  \n",
       "0            2                    data          22  \n",
       "1            2                    read          20  \n",
       "2            2             iainljbrown          19  \n",
       "3            2                     big          17  \n",
       "4            1             datascience          17  \n",
       "5            1                      ai          16  \n",
       "6            1         machinelearning          15  \n",
       "7            1                     iot          12  \n",
       "8            1                      ml           7  \n",
       "9            1  artificialintelligence           6  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "from collections import Counter\n",
    "\n",
    "authfile = 'auth.k'\n",
    "api = tw_oauth()\n",
    "\n",
    "df_results =  pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "Qye_words= ['bigdata']\n",
    "query_in = 'BIGDATA+BigData+BigData'\n",
    "\n",
    "# example of locations\n",
    "location_list = ['NY', 'London', 'Mumbai', 'Paris']\n",
    "query_list = [['NY', 'New York'], \n",
    "              ['London', 'london'],\n",
    "              ['Mumbai', 'mumbai', 'bombay', 'Bombay'],\n",
    "              ['paris', 'Paris']]\n",
    "\n",
    "# lets view frequent words about Bigdata in different cities\n",
    "for i in range(len(location_list)):\n",
    "    \n",
    "    location_words = query_list[i]\n",
    "\n",
    "    print('Retreiving tweets since: ', start_date, ' about', Qye_words[0] ,\n",
    "          ' Tweeted in locations: ',  location_list[i] )\n",
    "\n",
    "    words = tw_get_tweets(api,  query_in, Qye_words = Qye_words,\n",
    "                                      location = location_words,\n",
    "                                      geo = None,\n",
    "                                      num_tweets=2000)\n",
    "\n",
    "    # lets count and sort words\n",
    "    counts = Counter(words)\n",
    "    word_list=[]\n",
    "    freq_list=[]\n",
    "    \n",
    "    if (len(counts) > 0):\n",
    "        for item, frequency in counts.most_common(10):\n",
    "            word_list.append(item)\n",
    "            freq_list.append(frequency)\n",
    "\n",
    "\n",
    "        df_results[location_list[i] + '_words'] = word_list\n",
    "        df_results[location_list[i] + '_freq'] = freq_list  \n",
    "\n",
    "df_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets query more intersting staff\n",
    "Lest check the popular words in Tweets abount president Trump and Iran in different countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retreiving tweets since:  2018-4-9  about trump  Tweeted in locations:  US\n",
      "Retreiving tweets since:  2018-4-9  about trump  Tweeted in locations:  INDIA\n",
      "Retreiving tweets since:  2018-4-9  about trump  Tweeted in locations:  ENGLAND\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_words</th>\n",
       "      <th>US_freq</th>\n",
       "      <th>INDIA_words</th>\n",
       "      <th>INDIA_freq</th>\n",
       "      <th>ENGLAND_words</th>\n",
       "      <th>ENGLAND_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deal</td>\n",
       "      <td>436</td>\n",
       "      <td>deal</td>\n",
       "      <td>14</td>\n",
       "      <td>deal</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>149</td>\n",
       "      <td>nuclear</td>\n",
       "      <td>4</td>\n",
       "      <td>nuclear</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decision</td>\n",
       "      <td>80</td>\n",
       "      <td>syria</td>\n",
       "      <td>4</td>\n",
       "      <td>n't</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>obama</td>\n",
       "      <td>77</td>\n",
       "      <td>dow</td>\n",
       "      <td>3</td>\n",
       "      <td>speech</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>world</td>\n",
       "      <td>50</td>\n",
       "      <td>went</td>\n",
       "      <td>3</td>\n",
       "      <td>donald</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>go</td>\n",
       "      <td>50</td>\n",
       "      <td>breaks</td>\n",
       "      <td>3</td>\n",
       "      <td>fact</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kurteichenwald</td>\n",
       "      <td>38</td>\n",
       "      <td>celebrate</td>\n",
       "      <td>3</td>\n",
       "      <td>buckle</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>went</td>\n",
       "      <td>37</td>\n",
       "      <td>facts</td>\n",
       "      <td>3</td>\n",
       "      <td>reckless</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stocks</td>\n",
       "      <td>37</td>\n",
       "      <td>energy</td>\n",
       "      <td>3</td>\n",
       "      <td>checked</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>iranian</td>\n",
       "      <td>37</td>\n",
       "      <td>matter</td>\n",
       "      <td>3</td>\n",
       "      <td>nowthisnews</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         US_words  US_freq INDIA_words  INDIA_freq ENGLAND_words  ENGLAND_freq\n",
       "0            deal      436        deal          14          deal            82\n",
       "1         nuclear      149     nuclear           4       nuclear            31\n",
       "2        decision       80       syria           4           n't            19\n",
       "3           obama       77         dow           3        speech            16\n",
       "4           world       50        went           3        donald            15\n",
       "5              go       50      breaks           3          fact            14\n",
       "6  kurteichenwald       38   celebrate           3        buckle            14\n",
       "7            went       37       facts           3      reckless            14\n",
       "8          stocks       37      energy           3       checked            14\n",
       "9         iranian       37      matter           3   nowthisnews            14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_politics =  pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "# list of words that we don't want to count, because they are part of the query\n",
    "# lower case is enought, because we filter words in lower case\n",
    "Qye_words= ['trump', 'iran', 'president', 'us', 'america']\n",
    "\n",
    "# build a query with logic operators\n",
    "query_in ='(Trump AND Iran) OR (trump AND iran) OR (TRUMP AND IRAN)'\n",
    "\n",
    "# define location names\n",
    "location_list = ['US', 'INDIA', 'ENGLAND']\n",
    "\n",
    "# define list of string by wich tweets are filtered\n",
    "query_list = [['NY', 'New York', 'Washington', 'San Fransisco', 'US', 'USA'], \n",
    "              ['Mumbai', 'mumbai', 'bombay', 'Bombay', 'Delhi', \n",
    "               'India', 'Bangalore', 'INDIA'],\n",
    "                ['London', 'london', 'Liverpool', 'England', 'Manchester']]\n",
    "\n",
    "\n",
    "# lets view frequent words about Bigdata in different cities\n",
    "for i in range(len(location_list)):\n",
    "    \n",
    "    location_words = query_list[i]\n",
    "\n",
    "    print('Retreiving tweets since: ', start_date, ' about', Qye_words[0] ,\n",
    "          ' Tweeted in locations: ',  location_list[i] )\n",
    "\n",
    "    words = tw_get_tweets(api,  query_in, Qye_words = Qye_words,\n",
    "                                      location = location_words,\n",
    "                                      geo = None,\n",
    "                                      num_tweets=6000)\n",
    "\n",
    "    # lets count and sort words\n",
    "    counts = Counter(words)\n",
    "    word_list=[]\n",
    "    freq_list=[]\n",
    "    \n",
    "    if (len(counts) > 0):\n",
    "        for item, frequency in counts.most_common(10):\n",
    "            word_list.append(item)\n",
    "            freq_list.append(frequency)\n",
    "\n",
    "\n",
    "        df_politics[location_list[i] + '_words'] = word_list\n",
    "        df_politics[location_list[i] + '_freq'] = freq_list  \n",
    "\n",
    "df_politics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
